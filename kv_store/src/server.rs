// This file contains the implementation of the Server struct. The Server struct is responsible for handling incoming messages, sending outgoing messages, and updating the database based on the decided entries from the OmniPaxos instance. The run method of the Server struct is the main event loop that processes incoming messages, sends outgoing messages, and updates the database in a loop. The run method uses tokio::select! to handle multiple asynchronous tasks concurrently.

//Importing the necessary modules and structs
use crate::database::Database;
use crate::kv::KVCommand;
use crate::{
    network::{Message, Network},
    OmniPaxosKV,
    NODES,
    PID,
};
use omnipaxos::util::LogEntry;
use serde::{Deserialize, Serialize};
use std::time::Duration;
use tokio::time;
use std::collections::HashMap;

//Defines the types of responses the server can send back to a client
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum APIResponse {
    //Indicates that a new log index has been decided
    Decided(u64),
    //Represents the result of a GET operation on the kv store, where the first part is the key and the second part is the optional value (Can be None if the key does not exist)
    Get(String, Option<String>),
}

//Defines the Server struct
pub struct Server {
    pub omni_paxos: OmniPaxosKV, //OmniPaxos instance for the server
    pub network: Network, //Manages sending and receiving messages from other nodes or clients.
    pub database: Database, //Handles the actual storage and retrieval of key-value pairs.
    pub last_decided_idx: u64, //Tracks the last log entry that was "decided" (i.e., agreed upon by the consensus process) and applied to the database.
}

// Implementing the Server struct
impl Server {

    
    async fn process_incoming_msgs(&mut self) {
        let messages = self.network.get_received().await;
        for msg in messages {
            self.handle_message(msg).await;
        }
        
        // After processing messages, check if we need to update our database
        self.handle_decided_entries().await;
    }

    //Collects messages generated by OmniPaxos component that need to be sent to other nodes and sends them
    async fn send_outgoing_msgs(&mut self) {
        let messages = self.omni_paxos.outgoing_messages();
        for msg in messages {
            let receiver = msg.get_receiver();
            self.network
                .send(receiver, Message::OmniPaxosMsg(msg))
                .await;
        }
    }

    async fn handle_decided_entries(&mut self) {
        let new_decided_idx = self.omni_paxos.get_decided_idx();
        //Check if there are new decided entries
        if self.last_decided_idx < new_decided_idx as u64 {
            println!("New decided entries detected: current={}, new={}", self.last_decided_idx, new_decided_idx);
            
            // Try to read all decided entries from the beginning if we're recovering
            let start_idx = if self.last_decided_idx == 0 {
                0 // Read from the beginning if we're just starting
            } else {
                self.last_decided_idx as usize // Otherwise continue from where we left off
            };
            
            match self.omni_paxos.read_decided_suffix(start_idx) {
                Ok(decided_entries) => {
                    //Update the database with the decided entries, and save the current latest decided index
                    println!("Applying {} new entries to database (indices {}-{})", 
                             decided_entries.len(), 
                             self.last_decided_idx + 1, 
                             new_decided_idx);
                    
                    // Print the entries for debugging
                    for (i, entry) in decided_entries.iter().enumerate() {
                        println!("Entry {}: {:?}", start_idx + i, entry);
                    }
                    
                    self.update_database(decided_entries);
                    self.last_decided_idx = new_decided_idx as u64;
                    
                    /*** reply to client with new decided index ***/
                    let msg = Message::APIResponse(APIResponse::Decided(new_decided_idx as u64));
                    self.network.send(0, msg).await;
                    
                    // snapshotting
                    if new_decided_idx % 5 == 0 {
                        println!(
                            "Creating snapshot at index {}. Log before: {:?}",
                            new_decided_idx,
                            self.omni_paxos.read_decided_suffix(0).unwrap()
                        );
                        self.omni_paxos
                            .snapshot(Some(new_decided_idx), true)
                            .expect("Failed to snapshot");
                        println!(
                            "Snapshot created. Log after: {:?}\n",
                            self.omni_paxos.read_decided_suffix(0).unwrap()
                        );
                    }
                }
                Err(e) => {
                    println!("Error reading decided entries: {:?}", e);
                }
            }
        }
    }

    //Iterates over decided log entries and applies each command to the database. This is how the state of the database is eventually updated based on the decisions made by the consensus algorithm.
    fn update_database(&mut self, decided_entries: Vec<LogEntry<KVCommand>>) {
        for entry in decided_entries {
            match entry {
                LogEntry::Decided(cmd) => {
                    println!("Applying command to database: {:?}", cmd);
                    let result = self.database.handle_command(cmd.clone());
                    match cmd {
                        KVCommand::Put(key, _) => {
                            println!("PUT operation for key {} completed", key);
                        }
                        KVCommand::Get(key) => {
                            println!("GET operation for key {} returned: {:?}", key, result);
                        }
                        KVCommand::Delete(key) => {
                            println!("DELETE operation for key {} completed", key);
                        }
                    }
                }
                _ => {
                    println!("Skipping non-decided entry: {:?}", entry);
                }
            }
        }
        
        // Force database to flush to disk
        self.database.flush();
    }

    //Main loop of the server that processes incoming messages, sends outgoing messages, and updates the database based on decided entries.
    //The run method uses tokio::select! to handle multiple asynchronous tasks concurrently.
    //The biased; directive gives priority to the first branch (message processing).
    pub(crate) async fn run(&mut self) {
        // First, try to recover state from disk or other nodes
        self.recover_state().await;
        
        // After recovery, ensure we're fully synchronized before accepting commands
        println!("Performing post-recovery synchronization...");
        for _ in 0..5 {
            // Force multiple sync attempts to ensure we're caught up
            self.force_sync().await;
            time::sleep(Duration::from_millis(500)).await;
        }
        
        println!("Node fully initialized and ready to process commands");
        
        let mut msg_interval = time::interval(Duration::from_millis(1));
        let mut tick_interval = time::interval(Duration::from_millis(10));
        let mut reconnect_interval = time::interval(Duration::from_secs(5));
        let mut sync_interval = time::interval(Duration::from_secs(5)); // Reduced from 15 to 5 seconds
        
        loop {
            tokio::select! {
                biased;
                _ = msg_interval.tick() => {
                    self.process_incoming_msgs().await;
                    self.send_outgoing_msgs().await;
                    self.handle_decided_entries().await;
                },
                _ = tick_interval.tick() => {
                    self.omni_paxos.tick();
                },
                _ = reconnect_interval.tick() => {
                    self.network.check_and_reconnect().await;
                },
                _ = sync_interval.tick() => {
                    // Periodically force a full sync to ensure we haven't missed anything
                    self.force_sync().await;
                },
                else => (),
            }
        }
    }

    async fn recover_state(&mut self) {
        println!("Starting recovery process...");
        
        // First, check if we have any local state from persistent storage
        let local_decided_idx = self.omni_paxos.get_decided_idx();
        if local_decided_idx > 0 {
            println!("Found local persistent state up to index {}", local_decided_idx);
            
            // Apply all locally stored decided entries to our database
            if let Ok(entries) = self.omni_paxos.read_decided_suffix(0) {
                println!("Applying {} locally stored entries to database", entries.len());
                self.update_database(entries);
                self.last_decided_idx = local_decided_idx as u64;
            }
        } else {
            println!("No local persistent state found, will try to sync from peers");
        }
        
        // Give some time for network connections to establish
        time::sleep(Duration::from_secs(2)).await;
        
        // Request database sync from peers
        self.request_database_sync().await;
        
        // Try to catch up with the cluster by requesting the latest decided index
        // This will trigger OmniPaxos to sync the log
        for _ in 0..10 {
            self.omni_paxos.tick();
            self.send_outgoing_msgs().await;
            time::sleep(Duration::from_millis(100)).await;
            self.process_incoming_msgs().await;
            
            // Check if we've made progress
            let current_decided_idx = self.omni_paxos.get_decided_idx();
            if current_decided_idx > self.last_decided_idx as usize {
                println!("Recovered log up to index {}", current_decided_idx);
                
                // Apply any new entries we've learned about
                if let Ok(entries) = self.omni_paxos.read_decided_suffix(self.last_decided_idx as usize) {
                    println!("Applying {} newly synced entries to database", entries.len());
                    self.update_database(entries);
                    self.last_decided_idx = current_decided_idx as u64;
                }
                
                // If we've made progress, we can stop early
                break;
            }
        }
        
        // Final check of our recovery status
        let final_decided_idx = self.omni_paxos.get_decided_idx();
        if final_decided_idx > 0 {
            println!("Recovery complete: database synchronized up to index {}", final_decided_idx);
        } else {
            println!("Warning: Could not recover any entries. Starting with empty state.");
            println!("This is normal for a new cluster, but indicates a problem for an existing one.");
        }
    }

    async fn force_sync(&mut self) {
        println!("Performing periodic full sync...");
        
        // Force OmniPaxos to sync
        self.omni_paxos.tick();
        self.send_outgoing_msgs().await;
        
        // Wait a bit for responses
        time::sleep(Duration::from_millis(500)).await;
        self.process_incoming_msgs().await;
        
        // Check if we've learned about new entries
        let current_decided_idx = self.omni_paxos.get_decided_idx();
        if current_decided_idx as u64 > self.last_decided_idx {
            println!("Sync found new entries up to index {}", current_decided_idx);
            
            // Apply any new entries we've learned about
            if let Ok(entries) = self.omni_paxos.read_decided_suffix(self.last_decided_idx as usize) {
                println!("Applying {} newly synced entries to database", entries.len());
                self.update_database(entries);
                self.last_decided_idx = current_decided_idx as u64;
            }
        } else {
            println!("Sync complete - no new entries found");
        }
    }

    // Request a full database sync from peers
    async fn request_database_sync(&mut self) {
        println!("Requesting database sync from peers...");
        
        // Create a special message to request database sync
        let sync_request = Message::APIRequest(KVCommand::Get("__sync_request__".to_string()));
        
        // Send to all peers
        for peer in NODES.iter().filter(|&pid| *pid != *PID) {
            println!("Requesting database sync from peer {}", peer);
            self.network.send(*peer, sync_request.clone()).await;
        }
        
        // Wait for responses
        for _ in 0..5 {
            time::sleep(Duration::from_millis(500)).await;
            let messages = self.network.get_received().await;
            
            for msg in messages {
                if let Message::DatabaseSync(entries) = msg {
                    println!("Received database sync with {} entries", entries.len());
                    self.database.apply_batch(entries);
                    return; // Successfully received sync
                } else {
                    // Process other messages normally
                    self.handle_message(msg).await;
                }
            }
        }
        
        println!("Did not receive database sync from any peer");
    }
    
    // Handle a single message
    async fn handle_message(&mut self, msg: Message) {
        match msg {
            Message::APIRequest(kv_cmd) => match kv_cmd {
                KVCommand::Get(key) => {
                    // Special handling for ping messages
                    if key == "__ping__" {
                        println!("Received ping request, responding with pong");
                        let msg = Message::APIResponse(APIResponse::Get("__ping__".to_string(), Some("__pong__".to_string())));
                        // Send response to the sender (which is a peer, not the client)
                        // We need to determine which peer sent this
                        // For now, broadcast to all peers
                        for peer in NODES.iter().filter(|&pid| *pid != *PID) {
                            self.network.send(*peer, msg.clone()).await;
                        }
                        // Also send to client for debugging
                        self.network.send(0, msg).await;
                    } else if key == "__sync_request__" {
                        // Handle database sync request
                        println!("Received database sync request");
                        let entries = self.database.get_all_entries();
                        println!("Sending {} entries in database sync", entries.len());
                        
                        // Send database sync to all peers (we don't know which one requested it)
                        let sync_msg = Message::DatabaseSync(entries);
                        for peer in NODES.iter().filter(|&pid| *pid != *PID) {
                            self.network.send(*peer, sync_msg.clone()).await;
                        }
                    } else {
                        // Normal GET request
                        println!("Processing GET request for key: {}", key);
                        let value = self.database.handle_command(KVCommand::Get(key.clone()));
                        println!("GET result for key {}: {:?}", key, value);
                        let msg = Message::APIResponse(APIResponse::Get(key, value));
                        // Send response to client (0 is the clientID)
                        self.network.send(0, msg).await;
                    }
                }
                cmd => {
                    println!("Appending command to log: {:?}", cmd);
                    
                    // Try multiple times to append the command to ensure it succeeds
                    let mut success = false;
                    for attempt in 1..=3 {
                        match self.omni_paxos.append(cmd.clone()) {
                            Ok(_) => {
                                println!("Successfully appended command to log");
                                success = true;
                                break;
                            }
                            Err(e) => {
                                println!("Attempt {}: Failed to append command to log: {:?}", attempt, e);
                                // Force a tick and sync before retrying
                                self.omni_paxos.tick();
                                self.send_outgoing_msgs().await;
                                time::sleep(Duration::from_millis(100)).await;
                            }
                        }
                    }
                    
                    if !success {
                        println!("WARNING: Failed to append command after multiple attempts");
                    }
                    
                    // Force a tick and send messages to propagate this command
                    self.omni_paxos.tick();
                    self.send_outgoing_msgs().await;
                    
                    // Immediately check for decided entries to apply this command faster
                    self.handle_decided_entries().await;
                }
            },
            Message::OmniPaxosMsg(msg) => {
                println!("Received OmniPaxos message: {:?}", msg);
                self.omni_paxos.handle_incoming(msg);
                
                // Force a tick and send messages to respond to this message
                self.omni_paxos.tick();
                self.send_outgoing_msgs().await;
                
                // Immediately check for decided entries
                self.handle_decided_entries().await;
            },
            Message::DatabaseSync(entries) => {
                println!("Received database sync with {} entries", entries.len());
                self.database.apply_batch(entries);
                
                // After applying a database sync, force another sync to ensure we're fully caught up
                time::sleep(Duration::from_millis(100)).await;
                self.force_sync().await;
            },
            _ => {
                println!("Received unhandled message type: {:?}", msg);
            }
        }
    }
}
