// This file contains the implementation of the Server struct. The Server struct is responsible for handling incoming messages, sending outgoing messages, and updating the database based on the decided entries from the OmniPaxos instance. The run method of the Server struct is the main event loop that processes incoming messages, sends outgoing messages, and updates the database in a loop. The run method uses tokio::select! to handle multiple asynchronous tasks concurrently.

//Importing the necessary modules and structs
use crate::database::Database;
use crate::kv::KVCommand;
use crate::{
    network::{Message, Network},
    OmniPaxosKV,
};
use omnipaxos::util::LogEntry;
use serde::{Deserialize, Serialize};
use std::time::Duration;
use tokio::time;

//Defines the types of responses the server can send back to a client
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum APIResponse {
    //Indicates that a new log index has been decided
    Decided(u64),
    //Represents the result of a GET operation on the kv store, where the first part is the key and the second part is the optional value (Can be None if the key does not exist)
    Get(String, Option<String>),
}

//Defines the Server struct
pub struct Server {
    pub omni_paxos: OmniPaxosKV, //OmniPaxos instance for the server
    pub network: Network, //Manages sending and receiving messages from other nodes or clients.
    pub database: Database, //Handles the actual storage and retrieval of key-value pairs.
    pub last_decided_idx: u64, //Tracks the last log entry that was "decided" (i.e., agreed upon by the consensus process) and applied to the database.
}

// Implementing the Server struct
impl Server {

    
    async fn process_incoming_msgs(&mut self) {
        let messages = self.network.get_received().await;
        for msg in messages {
            match msg {
                Message::APIRequest(kv_cmd) => match kv_cmd {
                    KVCommand::Get(key) => {
                        // Special handling for ping messages
                        if key == "__ping__" {
                            println!("Received ping request, responding with pong");
                            let msg = Message::APIResponse(APIResponse::Get("__ping__".to_string(), Some("__pong__".to_string())));
                            // Send response to the sender (which is a peer, not the client)
                            // We need to determine which peer sent this
                            // For now, broadcast to all peers
                            for peer in NODES.iter().filter(|&pid| *pid != *PID) {
                                self.network.send(*peer, msg.clone()).await;
                            }
                            // Also send to client for debugging
                            self.network.send(0, msg).await;
                        } else {
                            // Normal GET request
                            println!("Processing GET request for key: {}", key);
                            let value = self.database.handle_command(KVCommand::Get(key.clone()));
                            println!("GET result for key {}: {:?}", key, value);
                            let msg = Message::APIResponse(APIResponse::Get(key, value));
                            // Send response to client (0 is the clientID)
                            self.network.send(0, msg).await;
                        }
                    }
                    cmd => {
                        println!("Appending command to log: {:?}", cmd);
                        match self.omni_paxos.append(cmd.clone()) {
                            Ok(_) => println!("Successfully appended command to log"),
                            Err(e) => println!("Failed to append command to log: {:?}", e),
                        }
                        
                        // Force a tick and send messages to propagate this command
                        self.omni_paxos.tick();
                        self.send_outgoing_msgs().await;
                    }
                },
                Message::OmniPaxosMsg(msg) => {
                    println!("Received OmniPaxos message: {:?}", msg);
                    self.omni_paxos.handle_incoming(msg);
                    
                    // Force a tick and send messages to respond to this message
                    self.omni_paxos.tick();
                    self.send_outgoing_msgs().await;
                }
                _ => {
                    println!("Received unhandled message type: {:?}", msg);
                }
            }
        }
        
        // After processing messages, check if we need to update our database
        self.handle_decided_entries().await;
    }

    //Collects messages generated by OmniPaxos component that need to be sent to other nodes and sends them
    async fn send_outgoing_msgs(&mut self) {
        let messages = self.omni_paxos.outgoing_messages();
        for msg in messages {
            let receiver = msg.get_receiver();
            self.network
                .send(receiver, Message::OmniPaxosMsg(msg))
                .await;
        }
    }

    async fn handle_decided_entries(&mut self) {
        let new_decided_idx = self.omni_paxos.get_decided_idx();
        //Check if there are new decided entries
        if self.last_decided_idx < new_decided_idx as u64 {
            println!("New decided entries detected: current={}, new={}", self.last_decided_idx, new_decided_idx);
            
            // Try to read all decided entries from the beginning if we're recovering
            let start_idx = if self.last_decided_idx == 0 {
                0 // Read from the beginning if we're just starting
            } else {
                self.last_decided_idx as usize // Otherwise continue from where we left off
            };
            
            match self.omni_paxos.read_decided_suffix(start_idx) {
                Ok(decided_entries) => {
                    //Update the database with the decided entries, and save the current latest decided index
                    println!("Applying {} new entries to database (indices {}-{})", 
                             decided_entries.len(), 
                             self.last_decided_idx + 1, 
                             new_decided_idx);
                    
                    // Print the entries for debugging
                    for (i, entry) in decided_entries.iter().enumerate() {
                        println!("Entry {}: {:?}", start_idx + i, entry);
                    }
                    
                    self.update_database(decided_entries);
                    self.last_decided_idx = new_decided_idx as u64;
                    
                    /*** reply to client with new decided index ***/
                    let msg = Message::APIResponse(APIResponse::Decided(new_decided_idx as u64));
                    self.network.send(0, msg).await;
                    
                    // snapshotting
                    if new_decided_idx % 5 == 0 {
                        println!(
                            "Creating snapshot at index {}. Log before: {:?}",
                            new_decided_idx,
                            self.omni_paxos.read_decided_suffix(0).unwrap()
                        );
                        self.omni_paxos
                            .snapshot(Some(new_decided_idx), true)
                            .expect("Failed to snapshot");
                        println!(
                            "Snapshot created. Log after: {:?}\n",
                            self.omni_paxos.read_decided_suffix(0).unwrap()
                        );
                    }
                }
                Err(e) => {
                    println!("Error reading decided entries: {:?}", e);
                }
            }
        }
    }

    //Iterates over decided log entries and applies each command to the database. This is how the state of the database is eventually updated based on the decisions made by the consensus algorithm.
    fn update_database(&mut self, decided_entries: Vec<LogEntry<KVCommand>>) {
        for entry in decided_entries {
            match entry {
                LogEntry::Decided(cmd) => {
                    println!("Applying command to database: {:?}", cmd);
                    let result = self.database.handle_command(cmd.clone());
                    match cmd {
                        KVCommand::Put(key, _) => {
                            println!("PUT operation for key {} completed", key);
                        }
                        KVCommand::Get(key) => {
                            println!("GET operation for key {} returned: {:?}", key, result);
                        }
                        KVCommand::Delete(key) => {
                            println!("DELETE operation for key {} completed", key);
                        }
                    }
                }
                _ => {
                    println!("Skipping non-decided entry: {:?}", entry);
                }
            }
        }
        
        // Force database to flush to disk
        self.database.flush();
    }

    //Main loop of the server that processes incoming messages, sends outgoing messages, and updates the database based on decided entries.
    //The run method uses tokio::select! to handle multiple asynchronous tasks concurrently.
    //The biased; directive gives priority to the first branch (message processing).
    pub(crate) async fn run(&mut self) {
        // First, try to recover state from disk or other nodes
        self.recover_state().await;
        
        let mut msg_interval = time::interval(Duration::from_millis(1));
        let mut tick_interval = time::interval(Duration::from_millis(10));
        let mut reconnect_interval = time::interval(Duration::from_secs(5));
        let mut sync_interval = time::interval(Duration::from_secs(15));
        
        loop {
            tokio::select! {
                biased;
                _ = msg_interval.tick() => {
                    self.process_incoming_msgs().await;
                    self.send_outgoing_msgs().await;
                    self.handle_decided_entries().await;
                },
                _ = tick_interval.tick() => {
                    self.omni_paxos.tick();
                },
                _ = reconnect_interval.tick() => {
                    self.network.check_and_reconnect().await;
                },
                _ = sync_interval.tick() => {
                    // Periodically force a full sync to ensure we haven't missed anything
                    self.force_sync().await;
                },
                else => (),
            }
        }
    }

    async fn recover_state(&mut self) {
        println!("Starting recovery process...");
        
        // First, check if we have any local state from persistent storage
        let local_decided_idx = self.omni_paxos.get_decided_idx();
        if local_decided_idx > 0 {
            println!("Found local persistent state up to index {}", local_decided_idx);
            
            // Apply all locally stored decided entries to our database
            if let Ok(entries) = self.omni_paxos.read_decided_suffix(0) {
                println!("Applying {} locally stored entries to database", entries.len());
                self.update_database(entries);
                self.last_decided_idx = local_decided_idx as u64;
            }
        } else {
            println!("No local persistent state found, will try to sync from peers");
        }
        
        // Give some time for network connections to establish
        time::sleep(Duration::from_secs(2)).await;
        
        // Try to catch up with the cluster by requesting the latest decided index
        // This will trigger OmniPaxos to sync the log
        println!("Attempting to sync with peers...");
        for i in 0..15 {
            println!("Sync attempt {}/15", i+1);
            self.omni_paxos.tick();
            self.send_outgoing_msgs().await;
            time::sleep(Duration::from_millis(200)).await;
            self.process_incoming_msgs().await;
            
            // Check if we've made progress
            let current_decided_idx = self.omni_paxos.get_decided_idx();
            if current_decided_idx > local_decided_idx {
                println!("Synced additional entries up to index {}", current_decided_idx);
                
                // Apply any new entries we've learned about
                if let Ok(entries) = self.omni_paxos.read_decided_suffix(self.last_decided_idx as usize) {
                    println!("Applying {} newly synced entries to database", entries.len());
                    self.update_database(entries);
                    self.last_decided_idx = current_decided_idx as u64;
                }
                
                // If we've made progress, we can stop early
                break;
            }
        }
        
        // Final check of our recovery status
        let final_decided_idx = self.omni_paxos.get_decided_idx();
        if final_decided_idx > 0 {
            println!("Recovery complete: database synchronized up to index {}", final_decided_idx);
        } else {
            println!("Warning: Could not recover any entries. Starting with empty state.");
            println!("This is normal for a new cluster, but indicates a problem for an existing one.");
        }
    }

    async fn force_sync(&mut self) {
        println!("Performing periodic full sync...");
        
        // Force OmniPaxos to sync
        self.omni_paxos.tick();
        self.send_outgoing_msgs().await;
        
        // Wait a bit for responses
        time::sleep(Duration::from_millis(500)).await;
        self.process_incoming_msgs().await;
        
        // Check if we've learned about new entries
        let current_decided_idx = self.omni_paxos.get_decided_idx();
        if current_decided_idx as u64 > self.last_decided_idx {
            println!("Sync found new entries up to index {}", current_decided_idx);
            
            // Apply any new entries we've learned about
            if let Ok(entries) = self.omni_paxos.read_decided_suffix(self.last_decided_idx as usize) {
                println!("Applying {} newly synced entries to database", entries.len());
                self.update_database(entries);
                self.last_decided_idx = current_decided_idx as u64;
            }
        } else {
            println!("Sync complete - no new entries found");
        }
    }
}
